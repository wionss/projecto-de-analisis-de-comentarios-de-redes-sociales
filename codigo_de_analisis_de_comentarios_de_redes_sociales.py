# -*- coding: utf-8 -*-
"""codigo de analisis de comentarios de redes sociales

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hr0sjQkUCTGCR-VACnvEt9kkNjvdPX22

# Analisis de bases de datos de comentarios en redes sociales

Glosario de contenido

1.importar librerias

2.habilitar la fuente de datos

2-1.definir la base de datos

2-2.revisar atributo de la base de datos (buscar nulos, tipos de datos, nombre de las columnas, id duplicadas, preview del data set)

2.3.revisar los 10 valores mas grandes de una categoria y revisar el sentimiento predonominante

2.4.revisar valores estadisticos de los datos como el promedio, desviacion estandar y creacion de las bases de datos separadas por setimiento de los comentarios

3.creacion de graficas

3.1.creacion de los histogramas

3.2.calculo de la matriz de correlacion

3.3.creacion de graficas de barras

3.4.graficas de tipo scatter

3.5.creacion de graficas de distribucion de comentarios por mes de publicacion

3.6.graficas de dispersion

3.7.calculo y creacion de las graficas de distancias, de acuerdo a los sentimientos de los comentarios

3.8.grafica tipo count plot para sentimientos en comentarios

4.analisis y procesamiento del texto

4.1.identificacion de las palabras, conteo y graficar la frecuencia de las mas repetidas

5.creacion de modelos de prediccion

5.1.predicion por medio de regresion logistica

5.2.creacion de modelos de prediccion de naivebayes

5.3.tokenizacion lematizacion del texto

5.4.eliminar stopwords de los comentarios

5.5.clasificacion de naivebayes sin stopwords

5.6.metodos de clasificacion alternativos

1. importar librerias
"""

# en esta seccion se importan algunas de la librerias necesarias para poder compilar el coldigo

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import spacy
#
#lematizar con spacy

# importa una libreria para poder ejecutar el codigo sin interrupciones

import warnings
warnings.filterwarnings('ignore')

# en esta seccion se importan algunas de la librerias necesarias para poder compilar el coldigo

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# en esta seccion se importan algunas de la librerias necesarias para poder compilar el coldigo
import numpy as np

# en esta seccion se importan algunas de la librerias necesarias para poder compilar el coldigo
from datetime import datetime

"""2. habilitar la fuente de la base de datos"""

# pide permiso para tener acceso al contenido de google drive
from google.colab import drive
drive.mount('/content/drive')

"""2.1 definir la base de datos"""

# carga la informacion del data set y lo guarda en la variable df

path = 'drive/MyDrive/synthetic_social_media_data.csv'
df = pd.read_csv(path)

"""2.2 revisar atributo de la base de datos (buscar nulos, tipos de datos, nombre de las columnas, id duplicadas, preview del data set)"""

# se realiza el conteo de valores nulos en todas las columnas y se imprime el valor
nulos_por_columna = df.isnull().sum()
print(nulos_por_columna)

# se presenta una preview del data set para poder las columna que conforman el dataset y tener una idea de que tipo de informacion hay en cada una de ellas

print("Dataset Preview:")
df.head()

# se busca si existen id duplicadas en la primera columna del data set con el fin de saber si estas son unicas, o existen multiples comentarios de una cuenta

df['Post ID'].duplicated().sum()

# se calcula el promedio del numero de comentarios en el data set como respuestas
average_commnents = df['Number of Comments'].mean()
print(f"el promedio del numero de likes en este conjunto de datos es: {average_commnents}")

"""2.3  revisar los 10 valores mas grandes de una categoria y revisar el sentimiento predonominante"""

# se crea un filtro del data set para poder mostar los 10 usuarios con la mayor cantidad de seguidores en orden descendente

top_10_sorted = df.sort_values(by='User Follower Count', ascending=False).head(10)
print(top_10_sorted)

# se realiza un conteo del conjunto que se filtro anteriormente para poder observar que sentimiento se repitio en mayor cantidad

sentiment_counts = top_10_sorted['Sentiment Label'].value_counts()
print(sentiment_counts)

# se crea un filtro del data set para poder mostar los 10 usuarios con la mayor cantidad de likes por comentario en orden descendente

top_10_likes = df.sort_values(by='Number of Likes', ascending=False).head(10)
print(top_10_likes)

# se realiza un conteo del conjunto que se filtro anteriormente para poder observar que sentimiento se repitio en mayor cantidad

sentiment_counts = top_10_likes['Sentiment Label'].value_counts()
print(sentiment_counts)

"""2.4 revisar valores estadisticos de los datos como el promedio, desviacion estandar y creacion de las bases de datos separadas por setimiento de los comentarios"""

# se filtra el set para poder observar un conjunto de usuarios con una gran cantidad de seguidores y likes por comentario
set_de_rango = df[(df['User Follower Count'] >= 6000) & (df['Number of Likes'] >= 400)]

#  se imprime el promedio del numero de comentarios de este data set

average_comments = set_de_rango['Number of Comments'].mean()
print(f"el promedio del numero de comentarios en este conjunto de datos es: {average_comments}")

# se generan tres data sets cada uno conteniendo datos correspondientes a una categoria de comentario

datos_negativos = df[df['Sentiment Label'] == "Negative"]
datos_positivos = df[df['Sentiment Label'] == "Positive"]
datos_neutrales = df[df['Sentiment Label'] == "Neutral"]

# imprime la media de los valores del numero de like, shares, comentario y seguidores del dataset de comentarios neutrales

media_columna_likes = datos_neutrales['Number of Likes'].mean()
print("Media de la columna 'Number of Likes':", media_columna_likes)
media_columna_Shares = datos_neutrales['Number of Shares'].mean()
print("Media de la columna 'Number of Shares':", media_columna_Shares)
media_columna_Comments = datos_neutrales['Number of Comments'].mean()
print("Media de la columna 'Number of Comments':", media_columna_Comments)
media_columna_follower = datos_neutrales['User Follower Count'].mean()
print("Media de la columna 'User Follower Count':", media_columna_follower)

# imprime la media de los valores del numero de like, shares, comentario y seguidores del dataset de comentarios negativos

media_columna_likes = datos_negativos['Number of Likes'].mean()
print("Media de la columna 'Number of Likes':", media_columna_likes)
media_columna_Shares = datos_negativos['Number of Shares'].mean()
print("Media de la columna 'Number of Shares':", media_columna_Shares)
media_columna_Comments = datos_negativos['Number of Comments'].mean()
print("Media de la columna 'Number of Comments':", media_columna_Comments)
media_columna_follower = datos_negativos['User Follower Count'].mean()
print("Media de la columna 'User Follower Count':", media_columna_follower)

# imprime la media de los valores del numero de like, shares, comentario y seguidores del dataset de comentarios positivos

media_columna_likes = datos_positivos['Number of Likes'].mean()
print("Media de la columna 'Number of Likes':", media_columna_likes)
media_columna_Shares = datos_positivos['Number of Shares'].mean()
print("Media de la columna 'Number of Shares':", media_columna_Shares)
media_columna_Comments = datos_positivos['Number of Comments'].mean()
print("Media de la columna 'Number of Comments':", media_columna_Comments)
media_columna_follower = datos_positivos['User Follower Count'].mean()
print("Media de la columna 'User Follower Count':", media_columna_follower)

# imprime informacion del dataset como el tipo de variable por columna y los nombres de las columnas

print("\nDataset Summary:")
print(df.info())

# se calcula la desviacion standar del numero de likes, comentarios, shares y seguidores

desviacion_estandar = np.std(df['Number of Likes'])
desviacion_estandar2 = np.std(df['Number of Comments'])
desviacion_estandar3 = np.std(df['Number of Shares'])
desviacion_estandar4 = np.std(df['User Follower Count'])
print("Desviación estándar de la columna 'Number of Likes':", desviacion_estandar)
print("Desviación estándar de la columna 'Number of Comments':", desviacion_estandar2)
print("Desviación estándar de la columna 'Number of Shares':", desviacion_estandar3)
print("Desviación estándar de la columna 'User Follower Count':", desviacion_estandar4)

"""3. creacion de las graficas

3.1 creacion de los histogramas
"""

# se grafica el histograma de los numeros de likes de los comentarios negativos
plt.figure(figsize=(10, 6))
sns.histplot(datos_negativos['Number of Likes'], bins=30, kde=True, color='blue')
plt.title('Distribucion de numero de likes en comentarios negativos')
plt.xlabel('numero de likes')
plt.show()

# se grafica el histograma de los numeros de likes de los comentarios positivos
plt.figure(figsize=(10, 6))
sns.histplot(datos_positivos['Number of Likes'], bins=30, kde=True, color='blue')
plt.title('Distribucion de numero de likes en comentarios positivos')
plt.xlabel('numero de likes')
plt.show()

# se grafica el histograma de los numeros de likes de los comentarios neutrales
plt.figure(figsize=(10, 6))
sns.histplot(datos_neutrales['Number of Likes'], bins=30, kde=True, color='blue')
plt.title('Distribucion de numero de likes en comentarios neutrales')
plt.xlabel('numero de likes')
plt.show()

# se grafica el histograma de los numeros de likes de los comentarios generales
plt.figure(figsize=(10, 6))
sns.histplot(df['Number of Likes'], bins=30, kde=True, color='blue')
plt.title('Distribucion de numero de likes en comentarios generales')
plt.xlabel('numero de likes')
plt.show()

# se grafica el histograma de los numeros de shares de los comentarios neutrales
plt.figure(figsize=(10, 6))
sns.histplot(datos_neutrales['Number of Shares'], bins=30, kde=True, color='blue')
plt.title('Distribucion de numero de shares en comentarios neutrales')
plt.xlabel('numero de shares')
plt.show()

# se grafica el histograma de los numeros de likes de los comentarios positivos
plt.figure(figsize=(10, 6))
sns.histplot(datos_positivos['Number of Shares'], bins=30, kde=True, color='blue')
plt.title('Distribucion de numero de shares en comentarios positivos')
plt.xlabel('numero de shares')
plt.show()

# se grafica el histograma de los numeros de shares de los comentarios negativos
plt.figure(figsize=(10, 6))
sns.histplot(datos_negativos['Number of Shares'], bins=30, kde=True, color='blue')
plt.title('Distribucion de numero de shares en comentarios negativos')
plt.xlabel('numero de shares')
plt.show()

# se grafica el histograma de los numeros de likes de los comentarios generales
plt.figure(figsize=(10, 6))
sns.histplot(df['Number of Shares'], bins=30, kde=True, color='blue')
plt.title('Distribucion de numero de shares en comentarios generales')
plt.xlabel('numero de shares')
plt.show()

# se grafica el histograma de los numeros de seguidores de los comentarios generales
plt.figure(figsize=(10, 6))
sns.histplot(df['User Follower Count'], bins=30, kde=True, color='blue')
plt.title('Distribucion de numero de seguidores en comentarios generales')
plt.xlabel('numero de seguidores')
plt.show()

# se grafica el histograma de los numeros de shares de los comentarios generales
plt.figure(figsize=(10, 6))
sns.histplot(df['Number of Comments'], bins=30, kde=True, color='blue')
plt.title('Distribucion de numero de shares en comentarios generales')
plt.xlabel('numero de comentarios')
plt.show()

"""3.2 calculo de la matriz de correlacion"""

# Calcular la matriz de correlación del data set de comentarios positivos, con el fin de saber la influencias que tiene una columna sobre otra.
# para ello se crea el set llamado nuevo_set que solo contiene las columnas con datos de tipo int64
nuevo_set = datos_positivos[['Number of Likes', 'Number of Comments', 'Number of Shares', 'User Follower Count']]
correlacion = nuevo_set.corr()
#print(correlacion)
sns.heatmap(correlacion, annot = True)
plt.show()

# Calcular la matriz de correlación del data set de comentarios negativos, con el fin de saber la influencias que tiene una columna sobre otra.
# para ello se crea el set llamado nuevo_set que solo contiene las columnas con datos de tipo int64
nuevo_set = datos_negativos[['Number of Likes', 'Number of Comments', 'Number of Shares', 'User Follower Count']]
correlacion = nuevo_set.corr()
#print(correlacion)
sns.heatmap(correlacion, annot = True)
plt.show()

# Calcular la matriz de correlación del data set de comentarios neutrales, con el fin de saber la influencias que tiene una columna sobre otra.
# para ello se crea el set llamado nuevo_set que solo contiene las columnas con datos de tipo int64
nuevo_set = datos_neutrales[['Number of Likes', 'Number of Comments', 'Number of Shares', 'User Follower Count']]
correlacion = nuevo_set.corr()
#print(correlacion)
sns.heatmap(correlacion, annot = True)
plt.show()

# Calcular la matriz de correlación del data set de comentarios generales, con el fin de saber la influencias que tiene una columna sobre otra.
# para ello se crea el set llamado nuevo_set que solo contiene las columnas con datos de tipo int64
nuevo_set = df[['Number of Likes', 'Number of Comments', 'Number of Shares', 'User Follower Count']]
correlacion = nuevo_set.corr()
#print(correlacion)
sns.heatmap(correlacion, annot = True)
plt.show()

"""3.3 creacion de graficas de barras"""

# genera una grafica de barras del data set de comentarios positivos para conocer que tipo de formato de pots tiene el mayor numero de likes.

sns.barplot(x=datos_positivos['Post Type'], y=datos_positivos['Number of Likes'], data=df)
plt.title('conteo de numero de likes por tipo de post - comentarios positivos')
plt.xlabel('tipo de post')
plt.ylabel('numero de likes')
plt.show()

# genera una grafica de barras del data set de comentarios negativos para conocer que tipo de formato de posts tiene el mayor numero de likes.

sns.barplot(x=datos_negativos['Post Type'], y=datos_negativos['Number of Likes'], data=df)
plt.title('conteo de numero de likes por tipo de post - comentarios negativos')
plt.xlabel('tipo de post')
plt.ylabel('numero de likes')
plt.show()

# genera una grafica de barras del data set de comentarios neutraless para conocer que tipo de formato de posts tiene el mayor numero de likes.

sns.barplot(x=datos_neutrales['Post Type'], y=datos_neutrales['Number of Likes'], data=df)
plt.title('conteo de numero de likes por tipo de post - comentarios neutrales')
plt.xlabel('tipo de post')
plt.ylabel('numero de likes')
plt.show()

# genera una grafica de barras del data set de comentarios generales para conocer que tipo de formato de posts tiene el mayor numero de likes.
sns.barplot(x=df['Sentiment Label'], y=df['Number of Likes'], data=df)
plt.title('conteo de numero de likes por tipo de post - comentarios generales')
plt.xlabel('tipo de post')
plt.ylabel('numero de likes')
plt.show()

"""3.4 graficas de tipo scatter"""

# se realiza la grafica tipo scatter de los comentarios positivos, usando la columna de datos de los numeros de likes y el numero de comentarios. para tratar de detectar una tendencia o una aglomeracion de los datos

plt.scatter(x=datos_positivos['Number of Comments'], y=datos_positivos['Number of Likes'])
plt.title('distribucion de numero de comentarios por numero de likes - comentarios positivos')
plt.xlabel('numero de comentarios')
plt.ylabel('numero de likes')
plt.show()

# se realiza la grafica tipo scatter de los comentarios negativos, usando la columna de datos de los numeros de likes y el numero de comentarios. para tratar de detectar una tendencia o una aglomeracion de los datos

plt.scatter(x=datos_negativos['Number of Comments'], y=datos_negativos['Number of Likes'])
plt.title('distribucion de numero de comentarios por numero de likes - comentarios negativos')
plt.xlabel('numero de comentarios')
plt.ylabel('numero de likes')
plt.show()

# se realiza la grafica tipo scatter de los comentarios neutrales, usando la columna de datos de los numeros de likes y el numero de comentarios. para tratar de detectar una tendencia o una aglomeracion de los datos

plt.scatter(x=datos_neutrales['Number of Comments'], y=datos_neutrales['Number of Likes'])
plt.title('distribucion de numero de comentarios por numero de likes - comentarios neutrales')
plt.xlabel('numero de comentarios')
plt.ylabel('numero de likes')
plt.show()

# se realiza la grafica tipo scatter de los comentarios generales, usando la columna de datos de los numeros de likes y el numero de comentarios. para tratar de detectar una tendencia o una aglomeracion de los datos


plt.scatter(x=df['Number of Comments'], y=df['Number of Likes'])
plt.title('distribucion de numero de comentarios por numero de likes - comentarios generales')
plt.xlabel('numero de comentarios')
plt.ylabel('numero de likes')
plt.show()

"""3.5 creacion de graficas de distribucion de comentarios por mes de publicacion"""

# se extraera la informacion de la columna "post date and time" separandola para poder filtrar la informacion por mes
# se reviso el dataset para observar el formato de la fecha, el cual era año, mes, dias, horas, minutos y segundos
# luego se define la funcion llamada extract_month() que recibe la variable x y regresa una variable para poder definir el mes



def extract_month(x):
    date = datetime.strptime(x, "%Y-%m-%d %H:%M:%S") # Changed the date format to match the data
    return date.month

months = ["January", "February", "March", "April",
         "May", "June", "July", "August",
          "September", "October", "November", "December"]

# se crea una nueva columna llamada month en el que se almacena una variable tipo entera correspondiente al mes en que se realizo el comentario

df["month"] = df["Post Date and Time"].apply(extract_month)

# se realiza un conteo de los valores de la columna month para poder conocer que mes repite mas y se asigna los numeros correspondientes a el mes corespondiente de la lista months
# y se lo grafica para conocer la cantidad de comentarios por mes

count = df["month"].value_counts()
count = count.sort_index()
fig, axes = plt.subplots()
count.plot(kind="bar", ax=axes)
for container in axes.containers:
    axes.bar_label(container)
axes.set_xticklabels([months[i-1] for i in count.index])
axes.set_yticklabels(())
axes.set_ylabel("cantidad de comentarios")
axes.set_xlabel("mes")
axes.set_title("cantidad de comentarios por mes")


plt.show()

"""3.6 graficas de dispersion"""

# se genera un grafica tipo scatter plot para los datos de comentarios postivos. con los numeros de likes en el eje x, y en el eje y el numero de shares. para los tres tipos de posts y para poder distingirlos
# se les otorga la paleta de colores viridis

plt.figure(figsize=(10, 6))
sns.scatterplot(df, x=datos_positivos['Number of Likes'], y=datos_positivos['Number of Shares'], hue=datos_positivos['Post Type'], palette='viridis')
plt.title('Dispersión de genero de likes vs shares - comentarios positivos')
plt.xlabel('likes')
plt.ylabel('shares')
plt.legend(title='post type')
plt.show()

# se genera un grafica tipo scatter plot para los datos de comentarios negativos. con los numeros de likes en el eje x, y en el eje y el numero de shares. para los tres tipos de posts y para poder distingirlos
# se les otorga la paleta de colores viridis

plt.figure(figsize=(10, 6))
sns.scatterplot(df, x=datos_negativos['Number of Likes'], y=datos_negativos['Number of Shares'], hue=datos_negativos['Post Type'], palette='viridis')
plt.title('Dispersión de genero de likes vs shares - comentarios negativos')
plt.xlabel('likes')
plt.ylabel('shares')
plt.legend(title='post type')
plt.show()

# se genera un grafica tipo scatter plot para los datos de comentarios neutrales. con los numeros de likes en el eje x, y en el eje y el numero de shares. para los tres tipos de posts y para poder distingirlos
# se les otorga la paleta de colores viridis

plt.figure(figsize=(10, 6))
sns.scatterplot(df, x=datos_neutrales['Number of Likes'], y=datos_neutrales['Number of Shares'], hue=datos_neutrales['Post Type'], palette='viridis')
plt.title('Dispersión de genero de likes vs shares - comentarios neutrales')
plt.xlabel('likes')
plt.ylabel('shares')
plt.legend(title='post type')
plt.show()

# se genera un grafica tipo scatter plot para los datos de comentarios generales. con los numeros de likes en el eje x, y en el eje y el numero de shares. para los tres tipos de posts y para poder distingirlos
# se les otorga la paleta de colores viridis

plt.figure(figsize=(10, 6))
sns.scatterplot(df, x=df['Number of Likes'], y=df['Number of Shares'], hue=df['Post Type'], palette='viridis')
plt.title('Dispersión de genero de likes vs shares - comentarios generales')
plt.xlabel('likes')
plt.ylabel('shares')
plt.legend(title='post type')
plt.show()

"""3.7 calculo y creacion de las graficas de distancias, de acuerdo a los sentimientos de los comentarios"""

from itertools import combinations

# se calcula el promedio del numero de likes para el grupo de comentarios negativos, neutrales y positivos

class_means = df.groupby('Sentiment Label')['Number of Likes'].mean()


# print("Promedios de las medidas de las flores por especie:")
class_means

# Calcular las distancias Manhattan y Euclidiana entre los promedios
distancias = {'Euclidiana': [], 'Manhattan': []}

# Combinaciones representan todos los pares posibles de especies
class_pairs = list(combinations(class_means.index, 2))

# print(species_means.index)
print(class_pairs)

for sp1, sp2 in class_pairs:
    # print(sp1, sp2)
    diff = class_means.loc[sp1] - class_means.loc[sp2]
    #print(diff.values)

    # Calculamos la diferencia de dos especies de flores basadas en el promedio
    dist_euclidiana = np.linalg.norm(diff)
    dist_manhattan = np.sum(np.abs(diff))
    print(dist_euclidiana)
    print(dist_manhattan)

    distancias['Euclidiana'].append(dist_euclidiana)
    distancias['Manhattan'].append(dist_manhattan)

# Crear un DataFrame con las distancias
distancias_df = pd.DataFrame(distancias, index=[f" {sp1} -  {sp2}" for sp1, sp2 in class_pairs])
print("\nDistancias entre los promedios de los sentimientos de los comentarios:")
# print(distancias_df)

# Graficar las distancias de los comentarios positivos, negativos y neutrales
distancias_df.plot(kind='bar', figsize=(15, 6), color=['skyblue', 'lightgreen'])
plt.title("Distancias entre los promedios de las clases de comentario")
plt.ylabel("Distancia")
plt.xlabel("Pares de clases")
plt.xticks(rotation=0)
plt.show()

"""3.8 grafica tipo count plot para sentimientos en comentarios"""

# se usa la grafica countplot de sns para que grafique la cantidad de comentarios neutrales, positivos y negativos que hay en el data set general.

sns.countplot(data=df,x=df['Sentiment Label'])
plt.title("conteo de numero de comentario por categoria de sentimento")

# se usa la grafica countplot de sns para que grafique la cantidad de posts de video, imagenes y texto que hay en el data set general.

sns.countplot(data=df,x=df['Post Type'])
plt.title("conteo de numero de comentario por categoria de contenido")

"""4. analisis y procesamiento del texto"""

!python -m spacy download en_core_web_sm

# se importa librerias para poder tokenizar el texto y remover stopwords del texto para que sea mas facil clasificar y lematizar

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Descargar 'punkt', un paquete necesario para la tokenización de texto.
nltk.download('punkt')

# Descargar las stopwords (palabras vacías) para el idioma inglés.
nltk.download('stopwords')

"""4.1 identificacion de las palabras, conteo y graficar la frecuencia de las mas repetidas"""

# se crea una copia del data set general.

all_data = df.copy()

# se divide el data set en dos partes que se usaran para test y entrenamiento

dataset_mitad = all_data.sample(frac=0.8, random_state=1)
segunda_mitad = all_data.drop(dataset_mitad.index)

# se especifica que se usara la version del idioma ingles de spacy
nlp = spacy.load("en_core_web_sm")

# Función para procesar y limpiar los nombres utilizando spaCy para tokenizarlos
def process_names(name):
	doc = nlp(name)
	tokens = [token.text for token in doc if not token.is_punct and not token.is_stop]
  #print(tokens)
	return tokens

# Aplicar la función a cada nombre y combinar los tokens en una lista
all_data['Name_Tokens'] = all_data['Post Content'].apply(process_names) #Dentro

all_tokens = [token for name_tokens in all_data['Name_Tokens'] for token in name_tokens]

# Contar la frecuencia de cada palabra
word_freq = Counter(all_tokens)

# Seleccionar las palabras más comunes
common_words = word_freq.most_common(15) #Seleccione el número de las palabras comunes: se recomienda 20, para ver los nombres mas reptidos segun su frecuencia.

# elimina es primer valor de common_words por que este correspondia al espacio en blanco
common_words.pop(0)

# Visualización de las palabras más comunes
plt.figure(figsize=(10, 6))
sns.barplot(x=[word[0] for word in common_words], y=[word[1] for word in common_words], palette='pastel')
plt.title('20 Palabras Más Comunes en los comentario generales')
plt.xlabel('Palabra')
plt.ylabel('Frecuencia')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# importa las librerias para poder calcular la regresion lineal y calcular los coeficientes del modelo

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

"""5. creacion de modelos de prediccion"""

# se muestra las columnas del conjunto nuevo_set
nuevo_set.head()

"""5.1 predicion por medio de regresion logistica"""

#se crea una variable llamada X que sera las variables independientes y la varariable dependiente. en esta parte no importa mucho cual escojamos debido a los bajos valores de la matriz de correlacion

X = nuevo_set.values  # Variables independientes
y = nuevo_set['Number of Likes'].values                               # Variable dependiente

# 3.se Crea y entrenar el modelo de regresion linieal
model = LinearRegression()
model.fit(X, y)

# 4. Obtener los coeficientes del modelo y los imprimimos
coeficientes = model.coef_
intercepto = model.intercept_

print(f'Intercepto (b0): {intercepto}')
print('Coeficientes:')
print(f'Pclass: {coeficientes[0]}')
print(f'Age: {coeficientes[1]}')
print(f'SibSp: {coeficientes[2]}')
print(f'Parch: {coeficientes[3]}')

# 5. Evaluación del modelo
# Hacer predicciones
y_pred = model.predict(X)

# Calcular el error cuadrático medio (MSE)
mse = mean_squared_error(y, y_pred)
print(f'Error Cuadrático Medio (MSE): {mse}')

# 6. Visualización
# Gráfico de dispersión de la relación entre Age y Fare
#plt.figure(figsize=(10, 6))

plt.scatter(df['User Follower Count'], y, color='blue', label='Datos Reales')
plt.scatter(df['User Follower Count'], y_pred, color='red', label='Predicciones', alpha=0.5)
plt.xlabel('numero de seguidores')
plt.ylabel('numero de likes')
plt.title('Predicción de numeros de likes en comentarios en relacion del numero de seguidores usando Regresión Lineal Múltiple')
plt.legend()
plt.show()

#se generan tres data sets los cuales tendran los datos tipo int 64. para los comentarios negativos, neutrales y positivos

set_positivo = datos_positivos[['Number of Likes','Number of Comments','Number of Shares','User Follower Count']]
set_negativo = datos_negativos[['Number of Likes','Number of Comments','Number of Shares','User Follower Count']]
set_neutral = datos_neutrales[['Number of Likes','Number of Comments','Number of Shares','User Follower Count']]

# luego se muestran las columnas de estos nuevos datasets

set_positivo.head()

set_negativo.head()

set_neutral.head()

# se genera dos sets para definir una variable dependiente y otra independiente con los datos de comentarios positivos
X_positivo = set_positivo[['Number of Comments','Number of Shares','User Follower Count']].values  # Variables independientes
y_positivo = set_positivo['Number of Likes'].values                               # Variable dependiente

# 3. Crear y entrenar el modelo de regresion linieal
model = LinearRegression()
model.fit(X_positivo, y_positivo)

# 4. Obtener los coeficientes del modelo

# se almacena el valor del coeficiente en la variable correspondiente a este modelo para luego imprimirla
coeficientes = model.coef_

intercepto = model.intercept_
# se imprime los resultados de los coeficientes del modelo

print(f'Intercepto (b0): {intercepto}')
print('Coeficientes:')
print(f'comments: {coeficientes[0]}')
print(f'shares: {coeficientes[1]}')
print(f'follower: {coeficientes[2]}')

# 5. Evaluación del modelo
# Hacer predicciones


y_pred = model.predict(X_positivo)

# Calcular el error cuadrático medio (MSE)
mse = mean_squared_error(y_positivo, y_pred)
print(f'Error Cuadrático Medio (MSE): {mse}')

# 6. Visualización
# Gráfico de dispersión de la relación entre Age y Fare


plt.scatter(set_positivo['Number of Shares'], y_positivo, color='blue', label='Datos Reales')
plt.scatter(set_positivo['Number of Shares'], y_pred, color='red', label='Predicciones', alpha=0.5)
plt.xlabel('numero de shares')
plt.ylabel('numero de likes')
plt.title('Predicción de numeros de likes en comentarios en relacion del numero de shares usando Regresión Lineal Múltiple')
plt.legend()
plt.show()

"""5.2 creacion de modelos de prediccion de naivebayes"""

# se crean data sets que contendran solo el comentario y la label correspondiente para entrenamiento

data_corto = df[['Post Content','Sentiment Label']]

# se muestra las columnas del nuevo set
data_corto.head()

# se crear un set para entrenamiento, un set para realizar tokenizacion y un set para realizar el test de entrenamiento

train_mas_corto = data_corto[50:550]
copia_train_mas_corto = train_mas_corto
set_for_test = data_corto[551:1000]

# se muestran las columnas de los sets recien creados
train_mas_corto.head()

set_for_test.head()

# instala las librerias para poder analizar el texto y clasificarlo

!pip install -U texblob
!python -m textblob.download_corpora

from textblob.classifiers import NaiveBayesClassifier

# se convierten los data sets de entrenamiento y test a listas
list_train = train_mas_corto.values.tolist()
list_test = set_for_test.values.tolist()

# se aplica el modelo de aprendizaje supervisado de naive bayes con la lista de entrenamiento
cl = NaiveBayesClassifier(list_train)

# se aplica el modelo de entrenamiento a un elemento de la lista de testeo y luego se imprime el resultado y el valor esperado para poder compararlos y saber si fue exitoso
result = cl.classify(list_test[3][0])
print(list_test[3][0])
print(list_test[3][1])
print(result)

# se imprime el valor de la presicion del modelo recien entrenado
cl.accuracy(list_test)

# muestra las caracteristicas mas importantes y revisa si esta presente
cl.show_informative_features(5)

"""5.3 tokenizacion lematizacion del texto"""

# se importan las librerias para tokenizar y lematizar la columna de comentarios
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# se descarga los modulos necesarios
nltk.download('punkt')
nltk.download('wordnet')

# se aplica el lematizador de palabras
lemmatizer = WordNetLemmatizer()

# se crea una columna en la que se tokeniza la columna de comentarios y tambien se crea una columna con los comentarios lematizado

train_mas_corto['tokenized_review'] = train_mas_corto['Post Content'].apply(word_tokenize)
train_mas_corto['lematized_review'] = train_mas_corto['tokenized_review'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])
# data['tokenized_review'] = data['tokenized_review'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])
train_mas_corto.head(10)

# se muestran los comentarios correspondientes a cada categoria
Chemistry_comments = train_mas_corto[train_mas_corto['Sentiment Label'] == 'Positive']['lematized_review']
Physics_comments = train_mas_corto[train_mas_corto['Sentiment Label'] == 'Negative']['lematized_review']
Biology_comments = train_mas_corto[train_mas_corto['Sentiment Label'] == 'Neutral']['lematized_review']
print(Chemistry_comments, Physics_comments, Biology_comments, )

"""5.4 eliminar stopwords de los comentarios"""

# importa las librerias para filtar las stop words de los comentarios originales
nltk.download('stopwords')
from nltk.corpus import stopwords

# se define el lenguaje de las stopwords que tiene que detectar. y en el ciclo landa se recorre la columna y se crea una nueva columna sin stopwords
stop_words = stopwords.words('english')
copia_train_mas_corto['comment no stopwords'] = copia_train_mas_corto['Post Content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))

# se muestra las columnas del nuevo set, con la columna sin stop words
copia_train_mas_corto.head()

"""5.5 clasificacion de naivebayes sin stopwords"""

set_sin_stopwords = copia_train_mas_corto[['comment no stopwords','Sentiment Label']]

set_sin_stopwords.info()

sin_stops_corto = data_corto[50:550]
sin_stops_sec= data_corto[551:1000]

list_train2 = sin_stops_corto.values.tolist()
list_test2 = sin_stops_sec.values.tolist()

cl2 = NaiveBayesClassifier(list_train2)

result = cl2.classify(list_test2[3][0])
print(list_test[3][0])
print(list_test[3][1])
print(result)

cl2.accuracy(list_test)

# Load the datasets
train_df = pd.read_csv('drive/MyDrive/Train_sen.csv')
test_df = pd.read_csv('drive/MyDrive/Test_sen.csv')

# Display the first few rows of the training dataset
train_df.head()

# Display the first few rows of the test dataset
test_df.head()

train_df.info()

test_df.info()

# Check for missing values in the training and test datasets
train_df.isnull().sum()

test_df.isnull().sum()

# Drop rows with missing values in the training dataset
train_df.dropna(subset=['Body', 'Sentiment Type'], inplace=True)
# Fill missing values in the test dataset with an empty string
test_df['Body'].fillna('', inplace=True)

"""5.6.metodos de clasificacion alternativos"""

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(train_df['Body'], train_df['Sentiment Type'], test_size=0.2, random_state=42)

# Vectorize the text data using TF-IDF
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_val_tfidf = tfidf_vectorizer.transform(X_val)
X_test_tfidf = tfidf_vectorizer.transform(test_df['Body'])

# Train a logistic regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train_tfidf, y_train)

# Predict on the validation set
y_val_pred = model.predict(X_val_tfidf)

# Calculate the accuracy of the model (el valor es mejor que la precision de la regresion logistica, pero seria adecuado que fuera un poco mayor)
accuracy = accuracy_score(y_val, y_val_pred)
print(f'Validation Accuracy: {accuracy:.2f}')

# grafica de confusion matrix para conocer el rendimiento del entrenamiento del modelo de clasificacion
conf_matrix = confusion_matrix(y_val, y_val_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Display the classification report
print(classification_report(y_val, y_val_pred))

# Predict on the test set
test_df['Sentiment Type'] = model.predict(X_test_tfidf)

# Display the first few rows of the test dataset with predictions
test_df.head()